{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nfrom keras.layers import *\nfrom keras import backend as K\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.models import Model,load_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau,EarlyStopping\n\nfrom scipy.misc import toimage,imresize\nfrom skimage import exposure\nfrom PIL import Image\nimport cv2","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Image generators"},{"metadata":{"trusted":true},"cell_type":"code","source":"spirals_train_folder = '../input/drawings/spiral/training'\nspirals_val_folder = '../input/drawings/spiral/testing'\nwaves_train_folder = '../input/drawings/wave/training'\nwaves_val_folder = '../input/drawings/wave/testing'\n\nbatch_size = 24\n\n# histogram equalizer\ndef eqz_plz(img):\n    return exposure.equalize_hist(img)\n\n\nspiral_datagen = ImageDataGenerator(rotation_range=360, # they're spirals.\n                                    width_shift_range=0.1,\n                                    height_shift_range=0.1,\n                                    brightness_range=(0.5,1.5),\n                                    shear_range=0.2,\n                                    zoom_range=0.2,\n                                    horizontal_flip=True,\n                                    preprocessing_function=eqz_plz,\n                                    vertical_flip=True)\n\nwave_datagen = ImageDataGenerator(rotation_range=5,\n                                  width_shift_range=0.1,\n                                  height_shift_range=0.1,\n                                  brightness_range=(0.5,1.5),\n                                  shear_range=0.2,\n                                  zoom_range=0.2,\n                                  horizontal_flip=True,\n                                  preprocessing_function=eqz_plz,\n                                  vertical_flip=True)\n\n\nspiral_train_generator = spiral_datagen.flow_from_directory(directory=os.path.abspath(spirals_train_folder),\n                                                            target_size=(256, 256),\n                                                            color_mode=\"grayscale\",\n                                                            batch_size=batch_size,\n                                                            class_mode=\"binary\",\n                                                            shuffle=True,\n                                                            seed=666)\n\nspiral_val_generator = spiral_datagen.flow_from_directory(directory=os.path.abspath(spirals_val_folder),\n                                                            target_size=(256, 256),\n                                                            color_mode=\"grayscale\",\n                                                            batch_size=batch_size,\n                                                            class_mode=\"binary\",\n                                                            shuffle=True,\n                                                            seed=710)\n\nwave_train_generator = wave_datagen.flow_from_directory(directory=os.path.abspath(waves_train_folder),\n                                                        target_size=(256, 512), # HxW in machine learning, WxH in computer vision\n                                                        color_mode=\"grayscale\",\n                                                        batch_size=batch_size,\n                                                        class_mode=\"binary\",\n                                                        shuffle=True,\n                                                        seed=420)\n\nwave_val_generator = wave_datagen.flow_from_directory(directory=os.path.abspath(waves_val_folder),\n                                                        target_size=(256, 512),\n                                                        color_mode=\"grayscale\",\n                                                        batch_size=batch_size,\n                                                        class_mode=\"binary\",\n                                                        shuffle=True,\n                                                        seed=420)","execution_count":2,"outputs":[{"output_type":"stream","text":"Found 72 images belonging to 2 classes.\nFound 30 images belonging to 2 classes.\nFound 72 images belonging to 2 classes.\nFound 30 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss',patience=12,min_lr=1e-9,verbose=1)\nearly_stop = EarlyStopping(monitor='val_loss',patience=16,verbose=1)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\n\ndef nopamine_model(mode):\n    if (mode == 'spirals') or (mode == 'spiral'):\n        input_layer = Input(shape=(256,256,1),name=f'{mode}_input_layer')\n    elif (mode == 'waves') or (mode == 'wave'):\n        input_layer = Input(shape=(256,512,1),name=f'{mode}_input_layer')\n\n    m1 = Conv2D(256,(5,5),dilation_rate=4,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='relu',padding='same')(input_layer)\n    p1 = MaxPool2D((9,9),strides=3)(m1)\n    m2 = Conv2D(128,(5,5),dilation_rate=2,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='relu',padding='same')(p1)\n    p2 = MaxPool2D((7,7),strides=3)(m2)\n    m3 = Conv2D(64,(3,3),kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001),activation='relu',padding='same')(p2)\n    p3 = MaxPool2D((5,5),strides=2)(m3)\n    g1=Dropout(0.2)(p3)\n    f1 = Flatten()(g1)\n    d1 = Dense(666,activation='relu')(f1)\n    g2=Dropout(0.5)(d1)\n    d2 = Dense(1,activation='sigmoid')(g2)\n    \n\n    \n    this_model = Model(input_layer,d2)\n    this_model.summary()\n    return this_model","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Network for spirals"},{"metadata":{"trusted":true},"cell_type":"code","source":"spiral_model = nopamine_model(mode='spirals') # early stopping epoch 89: val_loss 0.4796, val_acc 0.8274\nspiral_model.compile(optimizer=Adam(lr=3.15e-5), loss='binary_crossentropy', metrics=['accuracy'])","execution_count":5,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nspirals_input_layer (InputLa (None, 256, 256, 1)       0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 256, 256, 256)     6656      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 83, 83, 256)       0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 83, 83, 128)       819328    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 26, 26, 64)        73792     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 11, 11, 64)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 11, 11, 64)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 7744)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 666)               5158170   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 666)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 667       \n=================================================================\nTotal params: 6,058,613\nTrainable params: 6,058,613\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"result=spiral_model.fit_generator(spiral_train_generator,\n                           validation_data=spiral_val_generator,\n                           epochs=666,\n                           steps_per_epoch=(2000//batch_size),\n                           validation_steps=(800//batch_size),\n                           callbacks=[reduce_lr,early_stop],\n                           verbose=1)","execution_count":6,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/666\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/skimage/exposure/exposure.py:124: UserWarning: This might be a color image. The histogram will be computed on the flattened image. You can instead apply this function to each color channel.\n  warn(\"This might be a color image. The histogram will be \"\n","name":"stderr"},{"output_type":"stream","text":"83/83 [==============================] - 37s 448ms/step - loss: 0.9469 - acc: 0.4885 - val_loss: 0.9340 - val_acc: 0.5317\nEpoch 2/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.9287 - acc: 0.4920 - val_loss: 0.9183 - val_acc: 0.5041\nEpoch 3/666\n83/83 [==============================] - 30s 356ms/step - loss: 0.9132 - acc: 0.4885 - val_loss: 0.9042 - val_acc: 0.5099\nEpoch 4/666\n83/83 [==============================] - 29s 348ms/step - loss: 0.8977 - acc: 0.5166 - val_loss: 0.8908 - val_acc: 0.5082\nEpoch 5/666\n83/83 [==============================] - 29s 355ms/step - loss: 0.8868 - acc: 0.4970 - val_loss: 0.8793 - val_acc: 0.5000\nEpoch 6/666\n83/83 [==============================] - 29s 345ms/step - loss: 0.8740 - acc: 0.5221 - val_loss: 0.8684 - val_acc: 0.5391\nEpoch 7/666\n83/83 [==============================] - 29s 354ms/step - loss: 0.8646 - acc: 0.5100 - val_loss: 0.8572 - val_acc: 0.6012\nEpoch 8/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.8549 - acc: 0.5191 - val_loss: 0.8491 - val_acc: 0.5144\nEpoch 9/666\n83/83 [==============================] - 29s 354ms/step - loss: 0.8478 - acc: 0.5181 - val_loss: 0.8428 - val_acc: 0.4980\nEpoch 10/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.8350 - acc: 0.5532 - val_loss: 0.8416 - val_acc: 0.4877\nEpoch 11/666\n83/83 [==============================] - 30s 356ms/step - loss: 0.8267 - acc: 0.5542 - val_loss: 0.8266 - val_acc: 0.5714\nEpoch 12/666\n83/83 [==============================] - 29s 345ms/step - loss: 0.8152 - acc: 0.5658 - val_loss: 0.8178 - val_acc: 0.5823\nEpoch 13/666\n83/83 [==============================] - 30s 360ms/step - loss: 0.8001 - acc: 0.5974 - val_loss: 0.8104 - val_acc: 0.5813\nEpoch 14/666\n83/83 [==============================] - 29s 349ms/step - loss: 0.7756 - acc: 0.6335 - val_loss: 0.8052 - val_acc: 0.6111\nEpoch 15/666\n83/83 [==============================] - 29s 355ms/step - loss: 0.7667 - acc: 0.6305 - val_loss: 0.7769 - val_acc: 0.5992\nEpoch 16/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.7428 - acc: 0.6707 - val_loss: 0.7656 - val_acc: 0.6276\nEpoch 17/666\n83/83 [==============================] - 30s 360ms/step - loss: 0.7351 - acc: 0.6586 - val_loss: 0.7508 - val_acc: 0.6488\nEpoch 18/666\n83/83 [==============================] - 29s 344ms/step - loss: 0.7248 - acc: 0.6807 - val_loss: 0.7455 - val_acc: 0.6420\nEpoch 19/666\n83/83 [==============================] - 30s 360ms/step - loss: 0.6998 - acc: 0.6978 - val_loss: 0.7391 - val_acc: 0.6171\nEpoch 20/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.6837 - acc: 0.7013 - val_loss: 0.7230 - val_acc: 0.6440\nEpoch 21/666\n83/83 [==============================] - 30s 356ms/step - loss: 0.6747 - acc: 0.7149 - val_loss: 0.7052 - val_acc: 0.6786\nEpoch 22/666\n83/83 [==============================] - 29s 351ms/step - loss: 0.6762 - acc: 0.6998 - val_loss: 0.6953 - val_acc: 0.6584\nEpoch 23/666\n83/83 [==============================] - 30s 360ms/step - loss: 0.6671 - acc: 0.7164 - val_loss: 0.6545 - val_acc: 0.7262\nEpoch 24/666\n83/83 [==============================] - 29s 346ms/step - loss: 0.6444 - acc: 0.7390 - val_loss: 0.6899 - val_acc: 0.6543\nEpoch 25/666\n83/83 [==============================] - 30s 360ms/step - loss: 0.6472 - acc: 0.7254 - val_loss: 0.6968 - val_acc: 0.6409\nEpoch 26/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.6244 - acc: 0.7450 - val_loss: 0.6369 - val_acc: 0.7140\nEpoch 27/666\n83/83 [==============================] - 29s 355ms/step - loss: 0.6297 - acc: 0.7435 - val_loss: 0.6917 - val_acc: 0.6806\nEpoch 28/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.6113 - acc: 0.7525 - val_loss: 0.6868 - val_acc: 0.6749\nEpoch 29/666\n83/83 [==============================] - 30s 359ms/step - loss: 0.6108 - acc: 0.7490 - val_loss: 0.6579 - val_acc: 0.6865\nEpoch 30/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.6010 - acc: 0.7580 - val_loss: 0.6579 - val_acc: 0.6975\nEpoch 31/666\n83/83 [==============================] - 29s 353ms/step - loss: 0.5945 - acc: 0.7510 - val_loss: 0.6070 - val_acc: 0.7579\nEpoch 32/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.5819 - acc: 0.7530 - val_loss: 0.6462 - val_acc: 0.7037\nEpoch 33/666\n83/83 [==============================] - 29s 346ms/step - loss: 0.5762 - acc: 0.7701 - val_loss: 0.6198 - val_acc: 0.7143\nEpoch 34/666\n83/83 [==============================] - 30s 359ms/step - loss: 0.5740 - acc: 0.7696 - val_loss: 0.6174 - val_acc: 0.7160\nEpoch 35/666\n83/83 [==============================] - 29s 346ms/step - loss: 0.5603 - acc: 0.7882 - val_loss: 0.5703 - val_acc: 0.7540\nEpoch 36/666\n83/83 [==============================] - 29s 355ms/step - loss: 0.5683 - acc: 0.7736 - val_loss: 0.6145 - val_acc: 0.7140\nEpoch 37/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.5522 - acc: 0.7781 - val_loss: 0.5659 - val_acc: 0.7460\nEpoch 38/666\n83/83 [==============================] - 29s 353ms/step - loss: 0.5491 - acc: 0.7811 - val_loss: 0.5793 - val_acc: 0.7469\nEpoch 39/666\n83/83 [==============================] - 29s 349ms/step - loss: 0.5315 - acc: 0.7962 - val_loss: 0.6189 - val_acc: 0.7083\nEpoch 40/666\n83/83 [==============================] - 29s 355ms/step - loss: 0.5217 - acc: 0.7907 - val_loss: 0.6150 - val_acc: 0.7160\nEpoch 41/666\n83/83 [==============================] - 29s 345ms/step - loss: 0.5324 - acc: 0.7877 - val_loss: 0.5789 - val_acc: 0.7262\nEpoch 42/666\n83/83 [==============================] - 29s 354ms/step - loss: 0.5176 - acc: 0.8002 - val_loss: 0.5673 - val_acc: 0.7305\nEpoch 43/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.5041 - acc: 0.8027 - val_loss: 0.5344 - val_acc: 0.7917\nEpoch 44/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.4945 - acc: 0.8122 - val_loss: 0.5642 - val_acc: 0.7469\nEpoch 45/666\n83/83 [==============================] - 29s 353ms/step - loss: 0.4993 - acc: 0.8092 - val_loss: 0.5489 - val_acc: 0.7540\nEpoch 46/666\n83/83 [==============================] - 30s 357ms/step - loss: 0.5077 - acc: 0.7962 - val_loss: 0.5503 - val_acc: 0.7490\nEpoch 47/666\n83/83 [==============================] - 29s 344ms/step - loss: 0.4837 - acc: 0.8203 - val_loss: 0.5261 - val_acc: 0.7738\nEpoch 48/666\n83/83 [==============================] - 29s 354ms/step - loss: 0.4850 - acc: 0.8102 - val_loss: 0.5597 - val_acc: 0.7449\nEpoch 49/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.4749 - acc: 0.8163 - val_loss: 0.5295 - val_acc: 0.7758\nEpoch 50/666\n83/83 [==============================] - 29s 353ms/step - loss: 0.4909 - acc: 0.8037 - val_loss: 0.5244 - val_acc: 0.7716\nEpoch 51/666\n83/83 [==============================] - 29s 349ms/step - loss: 0.4663 - acc: 0.8243 - val_loss: 0.5193 - val_acc: 0.7778\nEpoch 52/666\n83/83 [==============================] - 29s 352ms/step - loss: 0.4790 - acc: 0.8087 - val_loss: 0.5046 - val_acc: 0.7798\nEpoch 53/666\n83/83 [==============================] - 29s 348ms/step - loss: 0.4536 - acc: 0.8368 - val_loss: 0.5355 - val_acc: 0.7758\nEpoch 54/666\n83/83 [==============================] - 30s 356ms/step - loss: 0.4566 - acc: 0.8389 - val_loss: 0.5341 - val_acc: 0.7737\nEpoch 55/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.4409 - acc: 0.8338 - val_loss: 0.5462 - val_acc: 0.7698\nEpoch 56/666\n83/83 [==============================] - 30s 356ms/step - loss: 0.4397 - acc: 0.8469 - val_loss: 0.5207 - val_acc: 0.7942\nEpoch 57/666\n83/83 [==============================] - 29s 349ms/step - loss: 0.4441 - acc: 0.8328 - val_loss: 0.4628 - val_acc: 0.8194\nEpoch 58/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.4411 - acc: 0.8368 - val_loss: 0.5009 - val_acc: 0.7778\nEpoch 59/666\n83/83 [==============================] - 29s 349ms/step - loss: 0.4309 - acc: 0.8343 - val_loss: 0.5039 - val_acc: 0.7817\nEpoch 60/666\n83/83 [==============================] - 29s 354ms/step - loss: 0.4307 - acc: 0.8353 - val_loss: 0.5062 - val_acc: 0.7963\nEpoch 61/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.4313 - acc: 0.8379 - val_loss: 0.4925 - val_acc: 0.8075\n","name":"stdout"},{"output_type":"stream","text":"Epoch 62/666\n83/83 [==============================] - 29s 355ms/step - loss: 0.4133 - acc: 0.8509 - val_loss: 0.4735 - val_acc: 0.7963\nEpoch 63/666\n83/83 [==============================] - 29s 348ms/step - loss: 0.4154 - acc: 0.8499 - val_loss: 0.4763 - val_acc: 0.8155\nEpoch 64/666\n83/83 [==============================] - 29s 349ms/step - loss: 0.4097 - acc: 0.8544 - val_loss: 0.5161 - val_acc: 0.7860\nEpoch 65/666\n83/83 [==============================] - 29s 354ms/step - loss: 0.4141 - acc: 0.8534 - val_loss: 0.4851 - val_acc: 0.7956\nEpoch 66/666\n83/83 [==============================] - 29s 351ms/step - loss: 0.4045 - acc: 0.8534 - val_loss: 0.4688 - val_acc: 0.8354\nEpoch 67/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.3986 - acc: 0.8584 - val_loss: 0.5021 - val_acc: 0.7956\nEpoch 68/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.4016 - acc: 0.8544 - val_loss: 0.4270 - val_acc: 0.8313\nEpoch 69/666\n83/83 [==============================] - 30s 356ms/step - loss: 0.4070 - acc: 0.8489 - val_loss: 0.4485 - val_acc: 0.8135\nEpoch 70/666\n83/83 [==============================] - 28s 342ms/step - loss: 0.3959 - acc: 0.8514 - val_loss: 0.4706 - val_acc: 0.8086\nEpoch 71/666\n83/83 [==============================] - 62s 749ms/step - loss: 0.3901 - acc: 0.8584 - val_loss: 0.4758 - val_acc: 0.8135\nEpoch 72/666\n83/83 [==============================] - 29s 351ms/step - loss: 0.3853 - acc: 0.8594 - val_loss: 0.4604 - val_acc: 0.8230\nEpoch 73/666\n83/83 [==============================] - 29s 348ms/step - loss: 0.3712 - acc: 0.8705 - val_loss: 0.4777 - val_acc: 0.8254\nEpoch 74/666\n83/83 [==============================] - 29s 355ms/step - loss: 0.3816 - acc: 0.8700 - val_loss: 0.4571 - val_acc: 0.8251\nEpoch 75/666\n83/83 [==============================] - 29s 345ms/step - loss: 0.3955 - acc: 0.8539 - val_loss: 0.4480 - val_acc: 0.8175\nEpoch 76/666\n83/83 [==============================] - 29s 353ms/step - loss: 0.3623 - acc: 0.8820 - val_loss: 0.4449 - val_acc: 0.8210\nEpoch 77/666\n83/83 [==============================] - 29s 348ms/step - loss: 0.3745 - acc: 0.8655 - val_loss: 0.4739 - val_acc: 0.8175\nEpoch 78/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.3832 - acc: 0.8635 - val_loss: 0.4747 - val_acc: 0.8189\nEpoch 79/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.3663 - acc: 0.8705 - val_loss: 0.4590 - val_acc: 0.8214\nEpoch 80/666\n83/83 [==============================] - 29s 353ms/step - loss: 0.3678 - acc: 0.8685 - val_loss: 0.4553 - val_acc: 0.8210\n\nEpoch 00080: ReduceLROnPlateau reducing learning rate to 3.149999974993989e-06.\nEpoch 81/666\n83/83 [==============================] - 28s 343ms/step - loss: 0.3381 - acc: 0.8906 - val_loss: 0.4113 - val_acc: 0.8433\nEpoch 82/666\n83/83 [==============================] - 29s 355ms/step - loss: 0.3602 - acc: 0.8695 - val_loss: 0.4300 - val_acc: 0.8251\nEpoch 83/666\n83/83 [==============================] - 29s 346ms/step - loss: 0.3527 - acc: 0.8730 - val_loss: 0.4282 - val_acc: 0.8353\nEpoch 84/666\n83/83 [==============================] - 29s 355ms/step - loss: 0.3366 - acc: 0.8896 - val_loss: 0.4341 - val_acc: 0.8333\nEpoch 85/666\n83/83 [==============================] - 29s 346ms/step - loss: 0.3275 - acc: 0.8961 - val_loss: 0.4688 - val_acc: 0.8095\nEpoch 86/666\n83/83 [==============================] - 29s 353ms/step - loss: 0.3432 - acc: 0.8941 - val_loss: 0.4589 - val_acc: 0.8107\nEpoch 87/666\n83/83 [==============================] - 29s 348ms/step - loss: 0.3484 - acc: 0.8815 - val_loss: 0.3982 - val_acc: 0.8274\nEpoch 88/666\n83/83 [==============================] - 29s 354ms/step - loss: 0.3459 - acc: 0.8835 - val_loss: 0.4425 - val_acc: 0.8169\nEpoch 89/666\n83/83 [==============================] - 29s 346ms/step - loss: 0.3257 - acc: 0.9016 - val_loss: 0.4652 - val_acc: 0.8075\nEpoch 90/666\n83/83 [==============================] - 29s 354ms/step - loss: 0.3433 - acc: 0.8911 - val_loss: 0.4467 - val_acc: 0.8354\nEpoch 91/666\n83/83 [==============================] - 29s 345ms/step - loss: 0.3488 - acc: 0.8835 - val_loss: 0.4398 - val_acc: 0.8294\nEpoch 92/666\n83/83 [==============================] - 29s 351ms/step - loss: 0.3301 - acc: 0.8961 - val_loss: 0.4490 - val_acc: 0.8148\nEpoch 93/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.3389 - acc: 0.8870 - val_loss: 0.4389 - val_acc: 0.8333\nEpoch 94/666\n83/83 [==============================] - 29s 353ms/step - loss: 0.3475 - acc: 0.8755 - val_loss: 0.4132 - val_acc: 0.8374\nEpoch 95/666\n83/83 [==============================] - 29s 344ms/step - loss: 0.3328 - acc: 0.8951 - val_loss: 0.4375 - val_acc: 0.8075\nEpoch 96/666\n83/83 [==============================] - 30s 358ms/step - loss: 0.3425 - acc: 0.8845 - val_loss: 0.4204 - val_acc: 0.8313\nEpoch 97/666\n83/83 [==============================] - 29s 348ms/step - loss: 0.3357 - acc: 0.8911 - val_loss: 0.4339 - val_acc: 0.8135\nEpoch 98/666\n83/83 [==============================] - 29s 352ms/step - loss: 0.3382 - acc: 0.8921 - val_loss: 0.4114 - val_acc: 0.8498\nEpoch 99/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.3449 - acc: 0.8825 - val_loss: 0.4628 - val_acc: 0.8234\n\nEpoch 00099: ReduceLROnPlateau reducing learning rate to 3.149999884044519e-07.\nEpoch 100/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.3382 - acc: 0.8845 - val_loss: 0.4136 - val_acc: 0.8498\nEpoch 101/666\n83/83 [==============================] - 29s 350ms/step - loss: 0.3342 - acc: 0.8966 - val_loss: 0.4234 - val_acc: 0.8373\nEpoch 102/666\n83/83 [==============================] - 29s 347ms/step - loss: 0.3421 - acc: 0.8906 - val_loss: 0.4102 - val_acc: 0.8374\nEpoch 103/666\n83/83 [==============================] - 29s 352ms/step - loss: 0.3231 - acc: 0.8981 - val_loss: 0.4606 - val_acc: 0.8194\nEpoch 00103: early stopping\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here's how to load/save\nspiral_model.save('/kaggle/working/model_spirals.h5')","execution_count":20,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}